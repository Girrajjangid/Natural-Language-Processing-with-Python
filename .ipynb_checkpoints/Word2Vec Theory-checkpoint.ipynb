{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "It is a method of representing words in numerical way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms such as **latent semantic analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW and N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (CBOW) or continuous skip-gram. \n",
    "\n",
    "**CBOW :** In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption).\n",
    "\n",
    "**Skip-GRAM :** In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note, CBOW is faster while skip-gram is slower but does a better job for infrequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization\n",
    "\n",
    "Results of word2vec training can be sensitive to parametrization. The following are some important parameters in word2vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Training algorithm\n",
    "\n",
    "A Word2vec model can be trained with **hierarchical softmax** and/or **negative sampling.** To approximate the **conditional log-likelihood** a model seeks to maximize, the hierarchical softmax method uses a **Huffman tree** to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the **log-likelihood** of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors. \n",
    "As training epochs increase, hierarchical softmax stops being useful.\n",
    "\n",
    "### 2.Sub-sampling\n",
    "\n",
    "High frequency words often provide little information. Words with frequency above a certain threshold may be subsampled to increase training speed.\n",
    "\n",
    "### 3.Dimensionality\n",
    "\n",
    "Quality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain will diminish.Typically, the dimensionality of the vectors is set to be between 100 and 1,000.\n",
    "\n",
    "### 4.Context window\n",
    "\n",
    "The size of the context window determines how many words before and after a given word would be included as context words of the given word. According to the authors' note, the recommended value is **10 for skip-gram** and **5 for CBOW.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and model quality\n",
    "\n",
    "The use of different model parameters and different corpus sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.\n",
    "\n",
    "In models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.\n",
    "\n",
    "Accuracy increases overall as the number of words used increases, and as the number of dimensions increases. Mikolov et al report that doubling the amount of training data results in an increase in computational complexity equivalent to doubling the number of vector dimensions.\n",
    "\n",
    "Altszyler et al. (2017) studied Word2vec performance in two semantic tests for different corpus size. They found that Word2vec has a steep learning curve, outperforming another word-embedding technique (LSA) when it is trained with medium to large corpus size (more than 10 million words). However, with a small training corpus LSA showed better performance. Additionally they show that the best parameter setting depends on the task and the training corpus. Nevertheless, for skip-gram models trained in medium size corpora, with 50 dimensions, a window size of 15 and 10 negative samples seems to be a good parameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does Word2Vec work?\n",
    "\n",
    "Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture: skip-gram (slower, better for infrequent words) vs CBOW (fast)\n",
    "\n",
    "the training algorithm: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)\n",
    "\n",
    "sub-sampling of frequent words: can improve both accuracy and speed for large data sets (useful values are in range 1e-3 to 1e-5)\n",
    "\n",
    "dimensionality of the word vectors: usually more is better, but not always\n",
    "\n",
    "context (window) size: for skip-gram usually around 10, for CBOW around 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
